# -*- coding: utf-8 -*-
"""final hpcc file.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rCl-Kst9OBXDjLKvlVukGE0d71dAIm7e
"""

import pandas as pd
import numpy as np
import spacy
import re
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from google.colab import drive
drive.mount('/content/drive')
import warnings; warnings.simplefilter('ignore')

# global variables

# path to dataset
path = '/content/drive/MyDrive/books.csv'  # substitute with the correct file path going to teh CSV File


# create empty dictionary to store predicted data types
predicted_data_types = {}

# type map for nlp predictions
type_map = {
            'org': ['name', 'brand', 'company', 'organization', 'firm', 'hotel', 'industry'],
            'person': ['name', 'relation', 'first name', 'last name', 'middle name'],
            'gpe': ['place', 'destination', 'origin', 'landmark', 'address', 'country', 'city', 'state'],
            'fac': ['building', 'airport', 'highway', 'bridge', 'station', 'metro', 'vehicle', 'hospital'],
            'norp': ['nationality', 'religion', 'political party', 'caste', 'creed', 'race', 'group', 'congregation'],
            'loc': ['location', 'place', 'mountain range', 'water body', 'coordinates'],
            'product': ['item', 'commodity', 'product', 'machine', 'technology', 'service', 'transportation', 'book', 'investment', 'insurance', 'description'],
            'event': ['sport', 'war', 'hurricane', 'tornado', 'attack', 'tsunami', 'calamity'],
            'work_of_art': ['movie', 'book', 'music', 'song', 'art', 'pop', 'culture', 'band'],
            'time': ['arrival', 'departure', 'time zone', 'standard time', 'expiration date', 'manufacture date', 'last seen', 'occurance', 'timing'],
            'date': ['ID','serial number','identification number', 'birthday', 'anniversary', 'expiration date', 'manufacture date'],
            'percent': ['percentage', 'ratio', 'range', 'factor', 'length', 'breadth', 'height', 'altitude', 'mean sea level', 'mean', 'median', 'sensex', 'statistics', 'nifty', 'rating', 'cross-section', 'dose', 'weight', 'number', 'enumeration'],
            'money': ['unit', 'worth', 'value', 'price', 'cost', 'expense', 'currency', 'quantity', 'amount'],
            'amount': ['percentage', 'ratio', 'range', 'factor', 'length', 'breadth', 'height', 'altitude', 'mean sea level', 'mean', 'median', 'sensex', 'statistics', 'nifty', 'rating', 'cross-section', 'dose', 'weight', 'number', 'enumeration'],
            'currency': ['unit', 'worth', 'value', 'price', 'cost', 'expense', 'currency', 'quantity', 'amount', 'gain', 'profit', 'loss', 'incomme', 'salary', 'revenue', 'net worth', 'net profit', 'net loss', 'capitol', 'debit', 'credit'],
            'measurement': ['unit', 'measurement', 'length', 'breadth', 'height', 'width', 'cross-section', 'quantity', 'amount', 'number of', 'enumeration', 'debit', 'credit'],
            'cardinal': ['ID', 'identification number', 'number plate', 'latitiude', 'longitude', 'coordinates', 'paper id', 'vehicle number', 'code', 'zipcode', 'contact number', 'shipping details', 'passcode', 'password', 'identity', 'email id', 'username', 'medicine', 'url'],
            'ordinal' : ['rating', 'numerical indicator', 'size', 'level', 'status'],
            'flag' : ['Yes','No','Male','Female', '0','1','Y','N','F','M']
}

"""This code defines a function called **classify_column_datatype** that takes a **pandas dataframe (df)** as input and returns a dictionary object containing the predicted data types for each column in the dataframe. The function iterates through each column in the dataframe and uses a series of regular expressions and checks to predict the data type of the column.


*   If the data type of a column is an object (i.e., a string), the function checks if the values in the column match a series of regular expressions to predict if it is a numerical integer, numerical float, datetime, time, or categorical data type. If none of the regular expressions match, the column is assumed to be categorical.

*   If the data type of a column is an integer, the function checks if there are any null values in the column. If there are null values, the column is assumed to be a float data type; otherwise, it is assumed to be an integer data type.

*   If the data type of a column is a float, the function checks if there are any null values in the column and if all values are integers. If there are null values, the column is assumed to be a float data type; if all values are integers, the column is assumed to be an integer data type; otherwise, it is assumed to be a float data type.

*   If the data type of a column is a boolean, the column is assumed to be a boolean data type.

*   If the data type of a column is anything else, the column is assumed to be of an unknown data type.

The function returns a dictionary object (predicted_data_types) containing the predicted data type for each column in the dataframe. After iterating through all columns in the dataframe, the function prints the predicted data types for each column.
"""

def classify_column_datatype(df):    
    # iterate through each column in the dataframe
    for column in df.columns:
        # check data type of column
        dtype = str(df[column].dtype)

        # predict data type based on dtype
        if dtype == 'object':
            if df[column].str.match(r'^\d+$').all():
                predicted_data_types[column] = 'numerical integer'

            elif df[column].str.match(r'^\d+\.\d+$').all():
                predicted_data_types[column] = 'numerical float'

            elif type(df[column][1]) == float:
              predicted_data_types[column] = 'float'

            elif type(df[column][1]) == int:
              predicted_data_types[column] = 'int'

            elif df[column].str.match(r'\b\d{2}/\d{2}/\d{4}\b').all():
                predicted_data_types[column] = 'datetime'
            
            elif df[column].str.match(r'\b\d{1,2}-\d{2}-\d{4}\b').all() or pd.to_datetime(df[column], errors='coerce').notnull().all():
                predicted_data_types[column] = 'datetime'

            elif df[column].str.match(r'\b\d{4}-\d{2}-\d{2}\b').all() or pd.to_datetime(df[column], errors='coerce').notnull().all():
                predicted_data_types[column] = 'datetime'

            elif df[column].str. match(r'\b\d{4}/\d{2}/\d{2}\b').all() or pd.to_datetime(df[column], errors='coerce').notnull().all():
                predicted_data_types[column] = 'datetime'

            elif df[column].str.match(r'\b\d{1,2}:\d{2}\b').all() or pd.to_datetime(df[column], errors='coerce').notnull().all():
                predicted_data_types[column] = 'datetime'

            elif df[column].str.match(r'\b\d{1,2} (am|pm)\b').all() or pd.to_datetime(df[column], errors='coerce').notnull().all():
                predicted_data_types[column] = 'time'    
            else:
                predicted_data_types[column] = 'categorical'
        elif dtype == 'int64':
            if df[column].isnull().any():
                predicted_data_types[column] = 'float'
            else:
                predicted_data_types[column] = 'int'
        elif dtype == 'float64':
            if df[column].isnull().any():
                predicted_data_types[column] = 'float'
            elif df[column].apply(lambda x: x.is_integer()).all():
                predicted_data_types[column] = 'int'
            else:
                predicted_data_types[column] = 'float'
        elif dtype == 'bool':
            predicted_data_types[column] = 'bool'
        else:
            predicted_data_types[column] = 'unknown'

    return predicted_data_types
    # print predicted data types
    for column, data_type in predicted_data_types.items():
        print(f"{column}: {data_type}")

"""This code defines a function called **preprocess_df** that takes in a file path as an argument. The function first reads the CSV file using **pd.read_csv** and skips the bad lines, if any. It then checks the shape of the dataframe using read1.shape. The function then checks for any missing values in the dataframe using **.isnull()**, and computes the sum of all missing values using **.sum()**. The missing values are then filled with empty strings using .fillna(), and the function checks for any remaining missing values in the dataframe.

The function then calls the classify_column_datatype function to predict the data types of the columns in the dataframe. The predicted data types are stored in a dictionary called** predicted_data_types**. **The function returns the cleaned dataframe (df_var) after all the preprocessing steps have been completed.**
"""

def preprocess_df(path):
  
  read1 = pd.read_csv(path , on_bad_lines='skip')
  read1.shape
  read1.isnull()
  read1.isnull().sum()
  df_var=read1.fillna(" ")
  df_var.isnull().sum()
  predicted_data_types = classify_column_datatype(df_var)
  # df_var.head()
  return df_var

# Applying preprocessing and displaying teh dataset
df1 = preprocess_df('/content/drive/MyDrive/HPCC_Datasets/books.csv')

df1

# print predicted data types
for column, data_type in predicted_data_types.items():
  print(f"{column}: {data_type}")

"""**get_entities(text)**: This function takes a text string as input and uses Spacy's pre-trained English language model to extract named entities from the text. It returns a list of named entity labels for the text.

**assign_column_ner_classification(df, num_samples)**: This function takes a pandas DataFrame df and an optional num_samples parameter to specify the number of samples to take from the DataFrame for classification. It applies the get_entities() function to each value in every column of the sampled DataFrame, and returns a new DataFrame with the same columns as the original, but where each value has been replaced with a list of named entity labels.

**major_col_pred_array(df, num_samples)**: This function takes a pandas DataFrame df and an optional num_samples parameter to specify the number of samples to take from the DataFrame for classification. It uses the assign_column_ner_classification() function to generate a new DataFrame where each value is a list of named entity labels. It then determines the most common named entity label for each column, and returns an array of the most common named entity label for each column.

The functions can be used to automatically classify the data in a DataFrame according to the types of named entities present in the data. The major_col_pred_array() function returns an array of the most common named entity label for each column, which can be used to infer the data type of each column.
"""

#NER on csv fILE applies on every value in a column and returns the column
nlp = spacy.load("en_core_web_sm")

def get_entities(text):
    doc = nlp(text)
    return [(ent.label_) for ent in doc.ents]

# This function assigns NER value to new copy of df and returns the new df
# Extracts majority of each column and stores in an array

def assign_column_ner_classification(df,num_samples=25):
  df = df.sample(n=num_samples)
  col_types_df = pd.DataFrame(df)
  for colname in df.columns:
    col_types_df[colname] = df[colname].astype(str).apply(get_entities)

  return col_types_df

def major_col_pred_array(df,num_samples=25):
    majority_arr = []
    new_df = pd.DataFrame()
    new_df = assign_column_ner_classification(df,num_samples)
    majority_arr = np.empty(new_df.shape[1], dtype='object')


    for i, col in enumerate(new_df.columns):
    
      counts = new_df[col].value_counts()
      
      
      if counts.empty:
        majority_arr[i] = None
      else:
        
        majority_arr[i] = counts.index[0]
      
      
      if majority_arr[i]:
        print(f"{col}: {majority_arr[i]}")
      else:
        print(f"{col}: None")
    
    return majority_arr

assign_column_ner_classification(df1,25)

# df storing the majority NER detected for each column, if not detected then column value = None
new_maj_col = major_col_pred_array(df1,25)

columns = new_maj_col.tolist()

"""The code provided consists of several functions that are used to detect and classify columns in a dataset based on their content. 

**Function: match_column_type(dataset, column_name)**
This function takes as input a dataset (a pandas dataframe) and a column name (a string) and returns the semantic type of the column. It does so by comparing the cosine similarity between the values in the specified column and a pre-defined type map that maps semantic types to example values. The semantic type with the highest average similarity score is returned.

**Function: predict_matching_values(df, column_name, key)**
This function takes as input a dataframe, a column name, and a key (a string). It returns a list of values that closely match the column name, based on the cosine similarity between the embeddings of the column name and the key.

**Function: detect1_semantic_type(values)**
This function takes as input a list of numeric values and returns the semantic type of the list. It does so by checking the range and differences between the values, as well as certain value ranges for specific semantic types.

**Function: classify_string_column_three(col_text)**
This function takes as input a string and attempts to classify it into one of several semantic types based on the presence of certain regex patterns. The semantic type with the highest confidence score is returned. The semantic types are: email, phone number, pincode, URL.

The **classify_columns( df )** code defines a function named classify_columns that takes in a Pandas DataFrame data and an optional argument batch_size. The function aims to classify the columns in the given CSV dataset using Named Entity Recognition (NER) and regex-based entity classification.

The function begins by selecting the first 30 rows of the given dataset using iloc and storing it in the data variable.

The function then initializes an empty list named predictions to store the predicted entity types of the columns.

Next, the function creates two lists of columns based on their predicted data types. The str_cols list stores all columns with a predicted data type of 'categorical', and the numeric_cols list stores all columns with a predicted data type of 'int' or 'numerical'.

After that, the function prints the lists of columns stored in str_cols and numeric_cols for debugging purposes.

The function then loops through each column in str_cols and tries to classify the column based on its text content using NER and regex. If no named entity is found, the function uses regex to classify the column. If the column is successfully classified, the function appends a dictionary to the predictions list containing the column name and its predicted entity type.

If the column cannot be classified based on the above methods, the function tries to match the column's name with a predefined list of column names and their corresponding entity types. If a match is found, the function appends a dictionary to the predictions list containing the column name and its predicted entity type.

If no match is found, the function appends a dictionary to the predictions list containing the column name and the value of the corresponding entity type in the predefined list.

Next, the function loops through each column in numeric_cols and tries to classify the column based on its range using a range-based hierarchical classification. If the column is successfully classified, the function appends a dictionary to the predictions list containing the column name and its predicted entity type.

If the column cannot be classified based on the above method, the function classifies the column as 'constant', 'binary', 'ordinal', 'interval', or 'ratio', based on its range. The function then appends a dictionary to the predictions list containing the column name and its predicted entity type.

Finally, the function returns a Pandas DataFrame containing the predicted entity types of the columns.
"""

def match_column_type(dataset, column_name):
    # Create a TfidfVectorizer object
    vectorizer = TfidfVectorizer()
    
    # Fit the vectorizer on the text data
    vectorizer.fit(dataset[column_name])
    
    # Create a dictionary to store the similarity scores
    similarity_scores = {}
    
    # Iterate over the type map
    for key, values in type_map.items():
        # Create a string by joining the values
        values_str = ' '.join(values)
        
        # Transform the values string and the text data into vectors
        values_vec = vectorizer.transform([values_str])
        text_vec = vectorizer.transform(dataset[column_name])
        
        # Compute the cosine similarity between the values vector and the text vectors
        similarity = cosine_similarity(values_vec, text_vec)
        
        # Compute the average similarity score
        avg_similarity = similarity.mean()
        
        # Store the average similarity score in the dictionary
        similarity_scores[key] = avg_similarity
    
    # Find the key with the highest similarity score
    best_match = max(similarity_scores, key=similarity_scores.get)
    
    return best_match

nlp = spacy.load("en_core_web_sm")

def predict_matching_values(df, column_name, key):
    
    num_columns = df.shape[1]
    num_columns=num_columns


    columns = new_maj_col.tolist()
    

    # Get the embeddings for the column name and key
    column_emb = nlp(str(column_name).lower()).vector.reshape(1, -1)
    key_emb = nlp(key).vector.reshape(1, -1)

    # Compute the cosine similarity between the column name and key embeddings
    sim = cosine_similarity(column_emb, key_emb)

    # Get the most similar value from the type map for the given key
    key_values = type_map[key]
    most_similar_value = max(key_values, key=lambda x: cosine_similarity(nlp(x).vector.reshape(1, -1), key_emb))

    # Create a list of values that closely match the column name
    matching_values = []
    for value in key_values:
        value_emb = nlp(value).vector.reshape(1, -1)
        value_sim = cosine_similarity(column_emb, value_emb)
        if value_sim >= sim:
          matching_values.append(value)

    return matching_values


def detect1_semantic_type(values):
    """Detects the semantic type of a list of numeric values."""
    
    # Initialize variables
    is_continuous = True
    is_discrete = True
    is_latitude = False
    is_longitude = False
    is_temperature = False
    is_idno = False
    is_altitude = False
    is_cost = False
    is_measurement = False
    is_salary = False
    
    # Calculate range and differences
    value_range = max(values) - min(values)
    value_diff = [values[i+1] - values[i] for i in range(len(values)-1)]
    diff_range = max(value_diff) - min(value_diff)
    # print(value_diff)
    # Check for discrete or continuous values
    if diff_range != 0:
        is_discrete = False
    else:
        is_continuous = False
    
    # # Check for latitude or longitude values
    if all(value >= -90 and value <= 90 and value > 10 for value in values):
        is_latitude = True
    elif all(value >= -180 and value <= 180 and value > 10 for value in values):
        is_longitude = True
        
    # Check for temperature values
    if value_range > 50 and all(value >= -10 and value <=50 for value in values):
        if all(abs(value) <= 10 for value in value_diff):
            is_temperature = True
        
    # Check for values
    if value_range <= 100 and all(value >= 0 and value <= 100 for value in values):
        is_idno = True
        
    # Check for altitude values
    if value_range > 1000 and all(value >= -413.6 and value <= 8848  for value in values):
        is_altitude = True
    
    # Check for cost values
    if value_range > 100 and any(value < 0 for value in values):
        is_cost = True
        
    # Check for measurement values
    if value_range > 1000 and any(value < 0 for value in values):
        is_measurement = True
        
    # Check for salary values
    if value_range > 1000 and all(value >= 0 for value in values):
        is_salary = True
        
    # Return results
    if is_latitude:
        return "Latitude"
    elif is_longitude:
        return "Longitude"
    elif is_temperature:
        return "Temperature"
    elif is_idno:
        return "ID number"
    elif is_altitude:
        return "Numeric Indicators"
    elif is_cost:
        return "Cost"
    elif is_measurement:
        return "Measurement"
    elif is_salary:
        return "Count"


def classify_string_column_three(col_text):
    # Define regex patterns for different entities
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    phone_pattern = r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'
    pincode_pattern = r'\b\d{6}\b'
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'

    # Check if column matches any of the patterns
    if re.search(email_pattern, col_text):
        return 'email'
    elif re.search(phone_pattern, col_text):
        return 'phone number'
    elif re.search(pincode_pattern, col_text):
        return 'pincode'
    elif re.search(url_pattern, col_text):
        return 'url'
    else:
        return None


def classify_columns(data, batch_size=1000):
    data = data.iloc[:30,:]
    """
    Classifies columns in a CSV dataset using NER and regex-based entity classification.

    :param data: A Pandas DataFrame containing the CSV dataset.
    :param batch_size: The size of the batches to use for processing the dataset.
    :return: A Pandas DataFrame with column names and their predicted entity types.
    """
    predictions = []

    str_cols = [col for col in predicted_data_types if 'categorical' in predicted_data_types[col]]
    numeric_cols = [col for col in predicted_data_types if 'int' in predicted_data_types[col] or 'numerical' in predicted_data_types[col]]
    print()
    print("printing the columns : ")
    print("str: ",str_cols)
    print("numeric : ",numeric_cols)
    print()
    # Classify object type columns using NLP and regex

    i = 0
    for col in str_cols:
        flag_tm = 0
        # If no named entity is found, use regex to classify the column
        col_type = None
        col_text = ' '.join(data[col].fillna('').values)
        col_type = classify_string_column_three(col_text)


        if col_type:
          predictions.append({'colname': col, 'prediction' : col_type})


        if not col_type:
            num_columns = data.shape[1]
            columns = []
            columns = new_maj_col.tolist()
            
            i = data.columns.get_loc(col)
            if columns[i]:
              match_value = new_maj_col.tolist()[i][0]
              col_type =  match_value
              #col_type = predict_matching_values(data, col, columns[i], columns)
              col_vals = [word for word in type_map[match_value.lower()]]
              predictions.append({'colname': col, 'prediction': col_vals})
              #print("based on column ner type = ",col, col_type, col_vals)
              
            else:    
              col_text_word = col_text.split()[0]
              # print(col_text_word)
              if re.search(r"[MF]", col_text_word, re.IGNORECASE):
                  col_type = 'Gender'
                  predictions.append({'colname': col, 'prediction': col_type})
              elif re.search(r"^[a-zA-Z]$", col_text_word, re.IGNORECASE):
                  col_type = 'Class'
                  predictions.append({'colname': col, 'prediction': col_type})
              elif re.search(r"^\d$", col_text_word, re.IGNORECASE):
                  col_type = 'Number|Count'
                  predictions.append({'colname': col, 'prediction': col_type})
              else:
                  key = new_maj_col.tolist()[df1.columns.get_loc(col)]
                  if key:
                    col_type = predict_matching_values(data, col, key[0])
                    predictions.append({'colname': col, 'prediction': col_type})

                  else:
                    predictions.append({'colname': col, 'prediction': key})
                                     
    # Classify numerical columns using range-based hierarchical classification
    for col in numeric_cols:
        if detect1_semantic_type(data[col]):
          col_type = detect1_semantic_type(data[col])
          predictions.append({'colname': col, 'prediction': col_type})
                     
        else:
            col_range = data[col].max() - data[col].min()
          
            if col_range == 0:
                col_type = 'constant'
            elif col_range == 1:
                col_type = 'binary'
            elif col_range <= 10:
                col_type = 'ordinal'
            elif col_range <= 100:
                col_type = 'interval'
            else:
                col_type = 'ratio'
            predictions.append({'colname': col, 'prediction': col_type})
                               
    return pd.DataFrame(predictions)

classify_columns(df1)



























